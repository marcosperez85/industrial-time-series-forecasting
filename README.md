# ğŸ­ Industrial Time Series Forecasting

Pipeline completo y modular para predecir valores futuros en series temporales industriales. DiseÃ±ado como un **template reutilizable** para experimentaciÃ³n rÃ¡pida, benchmarking de modelos y despliegue de prototipos, completamente **configurable a travÃ©s de un solo archivo YAML**.

## ğŸ¯ Objetivo

Desarrollar un sistema de predicciÃ³n **genÃ©rico y configurable** para series temporales industriales que permita:

- **AdaptaciÃ³n rÃ¡pida a nuevos datasets** editando solo `config.yaml`
- ExperimentaciÃ³n Ã¡gil con diferentes algoritmos de ML
- Feature engineering avanzado y automatizado
- ComparaciÃ³n objetiva y reproducible de modelos
- Despliegue inmediato a travÃ©s de API REST
- VisualizaciÃ³n y anÃ¡lisis automÃ¡tico de resultados

## âœ¨ CaracterÃ­sticas

### ğŸ”§ **ModularizaciÃ³n Completa**
- **Un solo archivo de configuraciÃ³n**: `config.yaml` controla todo el pipeline
- CÃ³digo reutilizable y sin valores hardcodeados
- Cambio de dataset sin modificar cÃ³digo fuente

### ğŸ¤– **Machine Learning Avanzado**
- **MÃºltiples algoritmos**: Linear Regression, Random Forest, Gradient Boosting
- **Feature Engineering automÃ¡tico**: lags, rolling statistics, interacciones, tÃ©rminos cuadrÃ¡ticos
- **ComparaciÃ³n automÃ¡tica** de modelos con mÃºltiples mÃ©tricas

### ğŸ“Š **AnÃ¡lisis y VisualizaciÃ³n**
- **EDA completamente genÃ©rico** que se adapta a cualquier dataset
- Visualizaciones automÃ¡ticas de patrones temporales
- AnÃ¡lisis de correlaciones y feature importance

### ğŸš€ **Despliegue y ProducciÃ³n**
- **API REST con FastAPI** para predicciones en tiempo real
- Scripts CLI para entrenamiento automatizado
- OrganizaciÃ³n estilo MLOps para escalabilidad

### ğŸ”„ **Reproducibilidad**
- Pipeline determinista y versionado
- Notebooks estructurados y modulares
- MÃ©tricas completas para evaluaciÃ³n objetiva

## ğŸ—ï¸ Arquitectura del Proyecto

```
industrial-time-series-forecasting/
â”‚
â”œâ”€â”€ config.yaml                           # ğŸ›ï¸  CONFIGURACIÃ“N CENTRAL
â”‚
â”œâ”€â”€ data/                                 # ğŸ“Š Datos del proyecto
â”‚   â”œâ”€â”€ raw/                             #     Datos originales
â”‚   â”‚   â””â”€â”€ industrial_timeseries.csv   #     Dataset generado/importado
â”‚   â””â”€â”€ processed/                       #     Datos con feature engineering
â”‚       â””â”€â”€ industrial_timeseries_featured.csv
â”‚
â”œâ”€â”€ models/                              # ğŸ¤– Modelos entrenados
â”‚   â”œâ”€â”€ best_model.pkl                   #     Mejor modelo seleccionado
â”‚   â”œâ”€â”€ model_info.pkl                  #     Metadatos y mÃ©tricas
â”‚   â”œâ”€â”€ linear_regression_model.pkl     #     Modelos individuales
â”‚   â”œâ”€â”€ random_forest_model.pkl         #
â”‚   â””â”€â”€ gradient_boosting_model.pkl     #
â”‚
â”œâ”€â”€ notebooks/                           # ğŸ““ AnÃ¡lisis y experimentaciÃ³n
â”‚   â”œâ”€â”€ 01_load.ipynb                   #     Carga y validaciÃ³n de datos
â”‚   â”œâ”€â”€ 02_eda.ipynb                    #     AnÃ¡lisis exploratorio genÃ©rico
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb    #     CreaciÃ³n de caracterÃ­sticas
â”‚   â”œâ”€â”€ 04_model.ipynb                  #     Entrenamiento y comparaciÃ³n
â”‚   â””â”€â”€ 05_forecast.ipynb               #     Predicciones futuras
â”‚
â”œâ”€â”€ src/                                 # ğŸ”§ CÃ³digo fuente modular
â”‚   â”œâ”€â”€ __init__.py                     #
â”‚   â”œâ”€â”€ config_loader.py                #     Carga de configuraciÃ³n YAML
â”‚   â”œâ”€â”€ data_loader.py                  #     Funciones de carga de datos
â”‚   â”œâ”€â”€ create_dataset.py               #     Generador de datos sintÃ©ticos
â”‚   â”œâ”€â”€ train_model.py                  #     Entrenamiento individual
â”‚   â”œâ”€â”€ model_compare.py                #     ComparaciÃ³n de modelos
â”‚   â”œâ”€â”€ predict.py                      #     Sistema de predicciones
â”‚   â”œâ”€â”€ main_api.py                     #     API REST con FastAPI
â”‚   â””â”€â”€ features/                       #     Motor de feature engineering
â”‚       â”œâ”€â”€ __init__.py                 #
â”‚       â””â”€â”€ feature_engineering.py      #     FeatureEngineeringEngine
â”‚
â”œâ”€â”€ requirements.txt                     # ğŸ“¦ Dependencias del proyecto
â””â”€â”€ README.md                           # ğŸ“– DocumentaciÃ³n principal
```

## ğŸš€ InstalaciÃ³n y Setup

### 1. Clonar el repositorio
```bash
git clone <repository-url>
cd industrial-time-series-forecasting
```

### 2. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 3. Generar dataset de ejemplo
```bash
cd src
python create_dataset.py
```

### 4. Verificar instalaciÃ³n
```bash
cd src
python load.py
```

## âš™ï¸ ConfiguraciÃ³n Central - config.yaml

Todo el comportamiento del pipeline se controla desde `config.yaml`:

```yaml
dataset:
  path: "data/raw/industrial_timeseries.csv"
  datetime_col: "timestamp"
  target_col: "value"
  freq: "H"
  raw_feature_columns:
    - temperature
    - demand_factor
    - operational_efficiency
    - energy_price

feature_engineering:
  time_features: true
  lags: [1, 24, 48, 168]
  rolling:
    windows: [24]
    functions: ["mean", "std"]
  interactions:
    - ["demand_factor", "operational_efficiency"]
    - ["temperature", "demand_factor"]
  squared_terms:
    - "temperature"

training:
  test_ratio: 0.2
  models:
    random_forest:
      n_estimators: 300
      max_depth: 10
    gradient_boosting:
      n_estimators: 300
      learning_rate: 0.05
```

### ğŸ”„ Cambiar de Dataset

Para usar un **nuevo dataset**, solo necesitas:

1. **Colocar tu CSV** en `data/raw/`
2. **Actualizar config.yaml**:
   ```yaml
   dataset:
     path: "data/raw/tu_dataset.csv"
     datetime_col: "fecha"           # Tu columna de tiempo
     target_col: "ventas"            # Tu variable objetivo
     raw_feature_columns:            # Tus features
       - precio
       - inventario  
       - promocion
   ```
3. **Ejecutar el pipeline** - todo se adapta automÃ¡ticamente

## ğŸ““ Uso de Notebooks

### Flujo Recomendado

```bash
# 1. Carga y validaciÃ³n inicial
jupyter notebook notebooks/01_load.ipynb

# 2. AnÃ¡lisis exploratorio automÃ¡tico  
jupyter notebook notebooks/02_eda.ipynb

# 3. Feature engineering configurable
jupyter notebook notebooks/03_feature_engineering.ipynb

# 4. Entrenamiento y comparaciÃ³n de modelos
jupyter notebook notebooks/04_model.ipynb

# 5. GeneraciÃ³n de predicciones futuras
jupyter notebook notebooks/05_forecast.ipynb
```

### CaracterÃ­sticas de los Notebooks

- **Completamente genÃ©ricos**: Se adaptan automÃ¡ticamente al dataset configurado
- **Sin cÃ³digo hardcodeado**: Todas las configuraciones vienen de `config.yaml`
- **AnÃ¡lisis automÃ¡tico**: EDA detecta tipos de datos y genera visualizaciones apropiadas
- **Reproducibles**: Mismos resultados en diferentes ejecuciones

## ğŸ”§ Scripts de LÃ­nea de Comandos

### Entrenamiento Individual
```bash
cd src

# Entrenar Random Forest (recomendado)
python train_model.py --model random_forest

# Entrenar Gradient Boosting  
python train_model.py --model gradient_boosting

# Entrenar Linear Regression
python train_model.py --model linear_regression
```

### ComparaciÃ³n AutomÃ¡tica de Modelos
```bash
cd src

# Solo comparar rendimiento
python model_compare.py

# Comparar y guardar mejor modelo
python model_compare.py --save
```

### ValidaciÃ³n de ConfiguraciÃ³n
```bash
cd src

# Verificar que config.yaml y datos son vÃ¡lidos
python load.py
```

## ğŸš€ API REST para ProducciÃ³n

### Iniciar servidor
```bash
cd src
python main_api.py
```

### Realizar predicciones
```bash
# PredicciÃ³n individual
curl -X POST "http://localhost:8000/predict" \
-H "Content-Type: application/json" \
-d '{
  "temperature": 22.5,
  "demand_factor": 0.75,
  "operational_efficiency": 0.85,
  "energy_price": 85.0,
  "hour": 14,
  "day_of_week": 2,
  "month": 6,
  "is_weekend": 0
}'

# Obtener informaciÃ³n del modelo
curl http://localhost:8000/model-info

# Health check
curl http://localhost:8000/health
```

### Endpoints Disponibles

- `GET /` - InformaciÃ³n general de la API
- `POST /predict` - Realizar predicciÃ³n individual
- `GET /health` - Estado del servicio
- `GET /model-info` - InformaciÃ³n del modelo cargado
- `GET /features` - Features requeridas por el modelo
- `GET /config` - ConfiguraciÃ³n actual del sistema
- `GET /docs` - DocumentaciÃ³n interactiva (Swagger UI)

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

El sistema utiliza **mÃºltiples mÃ©tricas** para evaluaciÃ³n exhaustiva:

### MÃ©tricas Principales
- **MAE** (Mean Absolute Error): Error promedio absoluto
- **RMSE** (Root Mean Square Error): Penaliza errores grandes
- **RÂ²** (R-squared): ProporciÃ³n de varianza explicada
- **MAPE** (Mean Absolute Percentage Error): Error porcentual promedio

### AnÃ¡lisis AutomÃ¡tico
- **DetecciÃ³n de Overfitting**: ComparaciÃ³n automÃ¡tica entre train/test
- **Feature Importance**: Ranking de variables mÃ¡s predictivas  
- **CorrelaciÃ³n con Target**: IdentificaciÃ³n de relaciones lineales
- **AnÃ¡lisis Temporal**: Patrones por hora, dÃ­a, mes, estacionalidad

### Ejemplo de Salida
```
ğŸ“Š COMPARACIÃ“N DE MODELOS:
================================================================================
Model               Train_MAE  Test_MAE  Train_R2  Test_R2   Test_RMSE  Test_MAPE
Linear Regression   15.23      18.45     0.8234    0.7891    24.67      2.34%
Random Forest       8.12       16.78     0.9456    0.8123    22.34      2.12% 
Gradient Boosting   6.89       15.23     0.9621    0.8345    20.45      1.98%

ğŸ† MEJORES MODELOS:
â€¢ Menor MAE: Gradient Boosting (MAE: 15.23)
â€¢ Mayor RÂ²: Gradient Boosting (RÂ²: 0.8345)  
â€¢ Menor MAPE: Gradient Boosting (MAPE: 1.98%)
```

## ğŸ” Casos de Uso

### Para Data Scientists
- **Prototipado rÃ¡pido** de modelos de forecasting
- **Benchmarking** de algoritmos en nuevos datasets
- **Feature engineering** sistemÃ¡tico y reproducible

### Para Ingenieros ML
- **Template base** para proyectos de series temporales
- **API lista para producciÃ³n** con validaciÃ³n automÃ¡tica
- **Pipeline CI/CD** compatible con herramientas estÃ¡ndar

### Para Analistas de Negocio
- **Predicciones automÃ¡ticas** sin conocimiento tÃ©cnico profundo
- **Dashboards** y visualizaciones auto-generadas
- **Interpretabilidad** de modelos y features

## ğŸ› ï¸ PersonalizaciÃ³n Avanzada

### Agregar Nuevos Modelos
```python
# En model_compare.py o train_model.py
from sklearn.svm import SVR

models['SVM'] = SVR(kernel='rbf', C=1.0)
```

### Nuevas Features de IngenierÃ­a
```yaml
# En config.yaml
feature_engineering:
  lags: [1, 6, 12, 24, 168]  # Agregar mÃ¡s lags
  rolling:
    windows: [6, 12, 24, 168]  # MÃºltiples ventanas
    functions: ["mean", "std", "min", "max"]  # MÃ¡s estadÃ­sticas
```

### Configurar para Nuevos Dominios
```yaml
# Ejemplo: Ventas de retail
dataset:
  path: "data/raw/retail_sales.csv"
  datetime_col: "date"
  target_col: "sales"
  raw_feature_columns:
    - price
    - inventory
    - promotion
    - competitor_price
```

## ğŸ“‹ Dependencias Principales

```txt
pandas>=1.5.0          # ManipulaciÃ³n de datos
numpy>=1.21.0          # CÃ¡lculos numÃ©ricos  
scikit-learn>=1.1.0    # Machine learning
matplotlib>=3.5.0      # VisualizaciÃ³n
seaborn>=0.11.0        # VisualizaciÃ³n estadÃ­stica
pyyaml>=6.0           # ConfiguraciÃ³n YAML
fastapi>=0.85.0       # API REST
joblib>=1.1.0         # SerializaciÃ³n de modelos
jupyter>=1.0.0        # Notebooks interactivos
```

## ğŸ¤ ContribuciÃ³n

1. Fork del proyecto
2. Crear branch para nueva feature (`git checkout -b feature/nueva-caracteristica`)
3. Commit de cambios (`git commit -m 'Agregar nueva caracterÃ­stica'`)
4. Push a la branch (`git push origin feature/nueva-caracteristica`)
5. Crear Pull Request

## ğŸ“ PrÃ³ximas Mejoras

- [ ] **Modelos de Deep Learning** (LSTM, GRU, Transformer)
- [ ] **AutoML** para selecciÃ³n automÃ¡tica de hiperparÃ¡metros  
- [ ] **DetecciÃ³n de anomalÃ­as** en tiempo real
- [ ] **Dashboard interactivo** con Streamlit/Dash
- [ ] **ContainerizaciÃ³n** con Docker
- [ ] **Monitoreo de deriva** de datos y modelos
- [ ] **Explicabilidad** con SHAP/LIME
- [ ] **Pipeline de CI/CD** completo

---

## ğŸ¯ Quick Start (5 minutos)

```bash
# 1. Clonar e instalar
git clone <repo>
cd industrial-time-series-forecasting  
pip install -r requirements.txt

# 2. Generar datos de ejemplo
cd src && python create_dataset.py

# 3. Ejecutar pipeline completo
jupyter notebook notebooks/01_load.ipynb      # Verificar datos
jupyter notebook notebooks/03_feature_engineering.ipynb  # Procesar  
jupyter notebook notebooks/04_model.ipynb     # Entrenar
python main_api.py                            # API REST

# 4. Tu sistema estÃ¡ listo! ğŸš€
```

**Â¿Preguntas?** Revisa los notebooks de ejemplo o abre un issue en el repositorio.